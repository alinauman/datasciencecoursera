trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
library(neuralnet)
nn <- neuralnet(Class ~., data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
library(neuralnet)
nn <- neuralnet(Class ~ Cl.thickness+Cell.size+Cell.shape, data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
library(neuralnet)
nn <- neuralnet(Class ~ Cl.thickness+Cell.size+Cell.shape,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testData, select = c("Cl.thickness","Cell.size", "Cell.shape"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
table(y_act,y_pred)
data(Ionosphere, package="mlbench")
ip <- Ionosphere[complete.cases(Ionosphere), ]
for(i in 1:9) {
ip[, i] <- as.numeric(as.character(ip[, i]))
}
ip$Class <- ifelse(ip$Class == "good", 1, 0)
ip$Class <- factor(ip$Class, levels = c(0, 1))
# Prep Training and Test data.
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(ip$Class, p=0.7, list = F)
trainData <- ip[trainDataIndex, ]
testData <- ip[-trainDataIndex, ]
table(trainData$Class)
#Neural Network
library(neuralnet)
nn <- neuralnet(Class ~ V5+V3+V27+V7+V8, data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testData, select = c("V5","V3", "V27", "V7", "V8"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
results
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
table(y_act,y_pred)
# Load data
# install.packages('mlbench')
data(BreastCancer, package="mlbench")
bc <- BreastCancer[complete.cases(BreastCancer), ]  # keep complete rows
# remove id column
bc <- bc[,-1]
# convert to numeric
for(i in 1:9) {
bc[, i] <- as.numeric(as.character(bc[, i]))
}
# Change Y values to 1's and 0's
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
# Prep Training and Test data.
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
# Class distribution of train data
table(trainData$Class)
set.seed(100)
down_train <- downSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(down_train$Class)
# Up Sample (optional)
set.seed(100)
up_train <- upSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(up_train$Class)
logitmod <- glm(Class ~., family = "binomial", data=down_train)
library(MASS)
steplogitmod <- stepAIC(logitmod, trace = FALSE)
coef(steplogitmod)
summary(logitmod)
library(MASS)
steplogitmod <- stepAIC(logitmod, trace = FALSE)
coef(steplogitmod)
summary(logitmod)
pred <- predict(logitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
pred <- predict(steplogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
data(BreastCancer, package="mlbench")
bc <- BreastCancer[complete.cases(BreastCancer), ]
# remove id column
bc <- bc[,-1]
for(i in 1:9) {
bc[, i] <- as.numeric(as.character(bc[, i]))
}
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(bc[,1:9], bc[,35], sizes=c(1:9), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(bc[,1:9], bc[,9], sizes=c(1:9), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
scaleddata<-scale(bc)
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
View(bc)
library(neuralnet)
nn <- neuralnet(Class ~ Mitoses,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
library(neuralnet)
nn <- neuralnet(Class ~ Mitoses,
data=trainData, hidden=c(1,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
library(neuralnet)
nn <- neuralnet(Class ~ Cl.thickness+Cell.size+Cell.shape,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
# install.packages('mlbench')
data(Ionosphere, package="mlbench")
ip <- Ionosphere[complete.cases(Ionosphere), ]
# convert to numeric
for(i in 1:9) {
ip[, i] <- as.numeric(as.character(ip[, i]))
}
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(ip[,1:34], ip[,35], sizes=c(1:34), rfeControl=control)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(ip[,1:34], ip[,35], sizes=c(1:34), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
ip$Class <- ifelse(ip$Class == "good", 1, 0)
ip$Class <- factor(ip$Class, levels = c(0, 1))
# Prep Training and Test data.
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(ip$Class, p=0.7, list = F)
trainData <- ip[trainDataIndex, ]
testData <- ip[-trainDataIndex, ]
# Class distribution of train data
table(trainData$Class)
library(neuralnet)
nn <- neuralnet(Class ~ V5+V3+V27+V7+V8,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testData, select = c("V5","V3", "V27", "V7", "V8"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
results
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
table(y_act,y_pred)
X_train = as.matrix(read.table('train2_5.txt'))
Y_train = as.matrix(read.table('train2_5Labels.txt'))
X_test = as.matrix(read.table('test2_5.txt'))
Y_test = as.matrix(read.table('test2_5Labels.txt'))
# Find the regression coefficients
regressionCoefficients <- function(X, Y, lambda){
add_one_matrix <- cbind(new_col=1, X)
transpose <- t(add_one_matrix)
a <- nrow(transpose)
iden <- diag(a)
new_matrix <- transpose%*%add_one_matrix
mult <- lambda*iden
sum <- new_matrix + mult
inverse_matrix <- solve(sum)
mult_new <- transpose%*%Y
final <- inverse_matrix%*%mult_new
#returnValue(final)
}
# Finding the prediction values
predictions <- function(testX,regressionCoefficients){
add_one_matrix <- cbind(new_col=1, testX)
predicted_labels <- add_one_matrix%*%regressionCoefficients
}
# Finding the confusion matrix
Matrix <- function(prediction,actualLabels){
TP=FP=TN=FN=0
for(i in seq_len(nrow(actualLabels)))
if(prediction[i]==2 && actualLabels[i]==2){
TP <- TP + 1
}else if(prediction[i]==5 && actualLabels[i]==5){
TN <- TN + 1
}else if(prediction[i]==2 && actualLabels[i]==5){
FP <- FP + 1
}else if(prediction[i]==5 && actualLabels[i]==2){
FN <- FN + 1
}
CM <- list("TP" = TP, "FP" = FP, "TN" = TN, "FN" = FN)
return(CM)
}
mylist<-list(1, 2, 3)
x <- c(2,4,3,5)
x[x(-1,2,3,4)]
x[(2,3,4)]
x <- c(12L,6L, 10L,8L)
median(x)
median(x).type
y <- median(x)
class(y)
x <- 5:8
names(x) <- letters[5:8]
x
v < 1:3
v <- 1:3
names(v) <- c("a","b","c")
v[4] <- -4
names(v[4])
x <- NA
y <- x/1
y
require("datasets")
df <- iris
plot(df)
2+2
1:100
print("Hello World!")
a <- 1
2 -> b
c <- d <- e <- 3
x <- c(1,2,5,9)
x
0:10
10:0
seq(10)
seq(30,0,by = -3)
(y <- c(5,1,0,10))
x + y
x * 2
sqrt(64)
log(100)
log10(100)
ls()
n1 <- 15
n1
typeof(n1)
n2 <- 1.5
n2
typeof(n2)
c1 <- "c"
c1
typeof(c1)
c1 <- "a string of text"
c1
typeof(c1)
l1 <- TRUE
l1
typeof(l1)
v1 <- c(1, 2, 3, 4, 5)
v1
is.vector(v1)
v2 <- c("a", "b", "c")
v2
is.vector(v2)
v3 <- c(TRUE, TRUE, FALSE, FALSE, TRUE)
v3
is.vector(v3)
m1 <- matrix(c(T, T, F, F, T, F), nrow = 2)
m1
m2 <- matrix(c("a", "b",
"c", "d"),
nrow = 2,
byrow = T)
m2
a1 <- array(c( 1:24), c(4, 3, 2))
a1
vNumeric   <- c(1, 2, 3)
vCharacter <- c("a", "b", "c")
vLogical   <- c(T, F, T)
df1 <- cbind(vNumeric, vCharacter, vLogical)
df1
df2 <- as.data.frame(cbind(vNumeric, vCharacter, vLogical))
df2
o1 <- c(1, 2, 3)
o2 <- c("a", "b", "c", "d")
o3 <- c(T, F, T, T, F)
list1 <- list(o1, o2, o3)
list1
list2 <- list(o1, o2, o3, list1)  # Lists within lists!
list2
View(list2)
(coerce1 <- c(1, "b", TRUE))
typeof(coerce1)
(coerce2 <- 5)
typeof(coerce2)
(coerce3 <- as.integer(5))
typeof(coerce3)
(coerce4 <- c("1", "2", "3"))
typeof(coerce4)
(coerce5 <- as.numeric(c("1", "2", "3")))
typeof(coerce5)
(coerce6 <- matrix(1:9, nrow= 3))
is.matrix(coerce6)
(coerce7 <- as.data.frame(matrix(1:9, nrow= 3)))
is.data.frame(coerce7)
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
# Load base packages manually
library(datasets)  # For example datasets
?iris
df <- iris
head(df)
hist(df$Sepal.Width,
# col  = "#CD0000",  # red3
# border = NA,  # No borders
main = "Histogram of Sepal Width",
xlab = "Sepal Width (in cm)")
hist(df$Sepal.Width,
col  = "#CD0000",  # red3
# border = NA,  # No borders
main = "Histogram of Sepal Width",
xlab = "Sepal Width (in cm)")
hist(df$Sepal.Width,
# col  = "#CD0000",  # red3
# border = NA,  # No borders
main = "Histogram of Sepal Width",
xlab = "Sepal Width (in cm)")
# THIS IS A LEVEL 1 HEADER #################################
## This Is a Level 2 Header ================================
### This is a level 3 header. ------------------------------
rm(list = ls())
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
# Clear mind :)
# Install pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")
mean(mtcars$mpg)
example <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), nrow = 4, ncol = 2)
View(example)
install.packages(c("ggplot2", "devtools", "lme4"))
library(ggplot2)
install.packages(c("backports", "broom", "fs", "mnormt", "openssl", "pillar", "pkgbuild", "processx", "Rcpp", "rlang", "tibble", "vctrs"))
install.packages("backports")
version
sessionInfo()
?devtools
??devtools
install.packages("KernSmooth")
library(KernSmooth)
library(swirl)
swirl()
Sys.Date()
mean(c(2, 4, 5))
submit()
boring_function('My first function')
boring_function('My first function!')
boring_function
submit()
my_mean(c(4, 5, 10))
submit()
submit()
submit()
remainder(5)
remainder(11, 5)
remainder(divisor = 11, num = 5)
remainder(4, div = 2)
args(remainder)
submit()
evaluate(sd, c(1.4, 3.6, 7.9, 8.8))
evaluate(function(x){x+1}, 6)
evaluate(function(x){x[length(x)]}, c(8, 4, 0))
evaluate(function(x){x[1]}, c(8, 4, 0))
evaluate(function(x){x[-1]}, c(8, 4, 0))
?paste
paste("Programming", "is", "fun!")
submit()
telegram("Data Science")
submit()
mad_libs(place="Lahore", adjective="surreal", noun="dog")
submit()
"I" %p% "love" %p% "R!"
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXlt(Sys.time())
class(t2)
t2
unclass(t2)
str(unclasst2)
str(unclass(t2))
t2$min
weekdays()
weekdays(d1)
months(t1)
quarters(t2)
t3 <- "October 17, 1986 08:24"
t4 <- strptime(t3, "%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(), t1, units = 'days')
cube <- function(x, n){}
cube <- function(x, n){
x^3
}
cube(3)
x <- 1:10
if(x>5){
x <- 0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
setwd("D:/Coursera/Data Science - Specialization - John Hopkin's/R Programming/Week_3")
?lapply
x <- list(a = 1:5, b = rnorm(10))
lapply(x, mean)
x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
lapply(x, mean)
x <- 1:4
lapply(x, runif)
lapply(x, runif, min = 0, max = 10)
x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
lapply(x, mean)
sapply(x, mean)
?apply
x <- matrix(rnorm(200), 20, 10)
apply(x, 2, mean)
apply(x, 1, sum)
apply(x, 1, quantile, probs = c(0.25, 0.75))
a <- array(rnorm(2*2*10), c(2, 2, 10))
apply(a, c(1,2), mean)
rowMeans(a, dims = 2)
